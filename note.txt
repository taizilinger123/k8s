###################################################
[root@node-2 k8s]# cat gitPush.sh 
#!/bin/bash 
DATE=`date +%Y%m%d`
git  add .
git commit -m  "$DATE"
git  push  git@github.com:taizilinger123/k8s.git 
##################################################
kubectl create deployment nginx --image=nginx
kubectl   get   pod   -A  -o  wide
kubectl   get  pod
kubectl  get  deployments.apps
[root@node-1 ~]# kubectl  get  deployments.apps 
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   1/1     1            1           45h
[root@node-1 ~]# kubectl  get  replicasets.apps 
NAME               DESIRED   CURRENT   READY   AGE
nginx-86c57db685   1         1         1       45h
[root@node-1 ~]# kubectl  get  rs 
NAME               DESIRED   CURRENT   READY   AGE
nginx-86c57db685   1         1         1       45h
[root@node-1 ~]# kubectl  scale -h 
Examples:
  # Scale a replicaset named 'foo' to 3.
  kubectl scale --replicas=3 rs/foo
[root@node-1 ~]# kubectl  scale deployment nginx  --replicas=2
deployment.apps/nginx scaled
[root@node-1 ~]# kubectl  get  pod 
NAME                     READY   STATUS    RESTARTS   AGE
nginx-86c57db685-mx59g   1/1     Running   0          11m
nginx-86c57db685-r67fx   1/1     Running   0          65s
[root@node-1 ~]# kubectl  get deployments.apps 
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   2/2     2            2           45h
[root@node-1 ~]# kubectl  get  pod  -o  wide 
NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx-86c57db685-mx59g   1/1     Running   0          11m   10.244.1.10   node-2   <none>           <none>
nginx-86c57db685-r67fx   1/1     Running   0          98s   10.244.2.17   node-3   <none>           <none>
[root@node-1 ~]# kubectl  set  image deployment/nginx nginx=nginx:1.19.7 --record
deployment.apps/nginx image updated
[root@node-1 ~]# kubectl   get  pod -w     #监控节点创建过程
NAME                     READY   STATUS              RESTARTS   AGE
nginx-5cf4d8f79-2cmgx    0/1     ContainerCreating   0          37s
nginx-86c57db685-mx59g   1/1     Running             0          16m
nginx-86c57db685-r67fx   1/1     Running             0          6m47s
[root@node-1 ~]# kubectl   get  pod  -o  wide 
NAME                     READY   STATUS              RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
nginx-5cf4d8f79-2cmgx    1/1     Running             0          2m5s    10.244.1.11   node-2   <none>           <none>
nginx-5cf4d8f79-vdh52    0/1     ContainerCreating   0          19s     <none>        node-3   <none>           <none>
nginx-86c57db685-r67fx   1/1     Running             0          8m15s   10.244.2.17   node-3   <none>           <none>
[root@node-1 ~]# curl  10.244.1.11
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
[root@node-1 ~]# curl  10.244.1.11/1
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.19.7</center>   ####这里能看到nginx镜像版本
</body>
</html>
[root@node-1 ~]# kubectl  describe deployments.apps  nginx 
OldReplicaSets:  <none>
NewReplicaSet:   nginx-5cf4d8f79 (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  59m   deployment-controller  Scaled up replica set nginx-86c57db685 to 2
  Normal  ScalingReplicaSet  53m   deployment-controller  Scaled up replica set nginx-5cf4d8f79 to 1
  Normal  ScalingReplicaSet  51m   deployment-controller  Scaled down replica set nginx-86c57db685 to 1
  Normal  ScalingReplicaSet  51m   deployment-controller  Scaled up replica set nginx-5cf4d8f79 to 2
  Normal  ScalingReplicaSet  50m   deployment-controller  Scaled down replica set nginx-86c57db685 to 0
[root@node-1 ~]# kubectl  rollout  history deployment  nginx 
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment/nginx nginx=nginx:1.19.7 --record=true
[root@node-1 ~]# kubectl  set  image deployment/nginx nginx=nginx:1.19.1 --record
deployment.apps/nginx image updated
[root@node-1 ~]# kubectl   get  pod   -w
NAME                     READY   STATUS              RESTARTS   AGE
nginx-5cf4d8f79-2cmgx    1/1     Running             0          62m
nginx-5cf4d8f79-vdh52    1/1     Running             0          60m
nginx-5dd4b6cddf-qxml2   0/1     ContainerCreating   0          8s
[root@node-1 ~]# kubectl  rollout  history deployment  nginx 
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment/nginx nginx=nginx:1.19.7 --record=true
3         kubectl set image deployment/nginx nginx=nginx:1.19.1 --record=true
[root@node-1 ~]# curl  10.244.2.19/1
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.19.1</center>   #这里看nginx的版本是1.19.1
</body>
</html>
[root@node-1 ~]# kubectl  rollout  history deployment  nginx 
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment/nginx nginx=nginx:1.19.7 --record=true     #看前面的数字2 revision
3         kubectl set image deployment/nginx nginx=nginx:1.19.1 --record=true  
[root@node-1 ~]# kubectl  rollout undo  deployment nginx  --to-revision=2
deployment.apps/nginx rolled back
[root@node-1 ~]# kubectl   get  pod  -w
NAME                     READY   STATUS              RESTARTS   AGE
nginx-5cf4d8f79-v4jhf    0/1     ContainerCreating   0          2s
nginx-5cf4d8f79-v87ps    1/1     Running             0          20s
nginx-5dd4b6cddf-969n6   1/1     Running             0          7m11s
nginx-5dd4b6cddf-qxml2   1/1     Terminating         0          8m33s
nginx-5dd4b6cddf-qxml2   0/1     Terminating         0          8m34s
nginx-5dd4b6cddf-qxml2   0/1     Terminating         0          8m35s
nginx-5dd4b6cddf-qxml2   0/1     Terminating         0          8m35s
nginx-5cf4d8f79-v4jhf    1/1     Running             0          20s
nginx-5dd4b6cddf-969n6   1/1     Terminating         0          7m30s
nginx-5dd4b6cddf-969n6   0/1     Terminating         0          7m32s
nginx-5dd4b6cddf-969n6   0/1     Terminating         0          7m33s
nginx-5dd4b6cddf-969n6   0/1     Terminating         0          7m33s
[root@node-1 ~]# kubectl   get  pod   -o wide 
NAME                    READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
nginx-5cf4d8f79-v4jhf   1/1     Running   0          4m8s    10.244.2.20   node-3   <none>           <none>
nginx-5cf4d8f79-v87ps   1/1     Running   0          4m26s   10.244.1.13   node-2   <none>           <none>
[root@node-1 ~]# curl  10.244.2.20/1
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.19.7</center>  #这里看nginx版本改为1.19.7了就是之前的版本
</body>
</html>
[root@node-1 ~]# kubectl  rollout  history deployment  nginx 
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none>
3         kubectl set image deployment/nginx nginx=nginx:1.19.1 --record=true
4         kubectl set image deployment/nginx nginx=nginx:1.19.7 --record=true
[root@node-1 ~]# kubectl  get  node 
NAME     STATUS   ROLES    AGE   VERSION
node-1   Ready    master   47h   v1.17.0
node-2   Ready    worker   47h   v1.17.0
node-3   Ready    worker   47h   v1.17.0
[root@node-1 ~]# kubectl  cordon node-1      #打个污点
node/node-1 cordoned
[root@node-1 ~]# 
[root@node-1 ~]# kubectl  get  node 
NAME     STATUS                     ROLES    AGE   VERSION
node-1   Ready,SchedulingDisabled   master   47h   v1.17.0
node-2   Ready                      worker   47h   v1.17.0
node-3   Ready                      worker   47h   v1.17.0
[root@node-1 ~]# kubectl  cordon node-2
node/node-2 cordoned
[root@node-1 ~]# kubectl  get  node 
NAME     STATUS                     ROLES    AGE   VERSION
node-1   Ready,SchedulingDisabled   master   47h   v1.17.0
node-2   Ready,SchedulingDisabled   worker   47h   v1.17.0
node-3   Ready                      worker   47h   v1.17.0
[root@node-1 ~]# kubectl  drain  -h
[root@node-1 ~]# kubectl  drain  node-2 --ignore-daemonsets --force  --delete-local-data 
node/node-2 already cordoned
evicting pod "metrics-server-6589f84c7-xq8bq"
pod/metrics-server-6589f84c7-xq8bq evicted
node/node-2 evicted
[root@node-1 ~]# kubectl  get  pod  -o wide 
NAME                    READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
nginx-5cf4d8f79-v4jhf   1/1     Running   0          45m     10.244.2.20   node-3   <none>           <none>
nginx-5cf4d8f79-ztnz7   1/1     Running   0          5m42s   10.244.2.21   node-3   <none>           <none>             #可以看到都在node-3这个节点上了，打了污点和驱逐node-2
[root@node-1 ~]# kubectl  get  node 
NAME     STATUS                     ROLES    AGE   VERSION
node-1   Ready,SchedulingDisabled   master   47h   v1.17.0
node-2   Ready,SchedulingDisabled   worker   47h   v1.17.0
node-3   Ready                      worker   47h   v1.17.0
[root@node-1 ~]# kubectl  uncordon node-2            #去除打的污点
node/node-2 uncordoned
[root@node-1 ~]# kubectl  get  node 
NAME     STATUS                     ROLES    AGE   VERSION
node-1   Ready,SchedulingDisabled   master   47h   v1.17.0
node-2   Ready                      worker   47h   v1.17.0
node-3   Ready                      worker   47h   v1.17.0
[root@node-1 ~]# kubectl  create deployment nginx --image=nginx --dry-run -o yaml    #yaml文件编写格式生成
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
[root@node-1 ~]# kubectl  create deployment nginx --image=nginx --dry-run -o yaml > nginx.yaml 
[root@node-1 ~]# cat  nginx.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
[root@node-1 ~]# kubectl  delete  deployment  nginx 
deployment.apps "nginx" deleted
[root@node-1 ~]# kubectl  get  rs
No resources found in default namespace.
[root@node-1 ~]# kubectl  get   pod 
No resources found in default namespace.
[root@node-1 ~]# kubectl  get  deployments.apps 
No resources found in default namespace.
##########################################################
[root@node-1 k8s.yaml]# cat  nginx.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
[root@node-1 k8s.yaml]# kubectl  apply  -f  nginx.yaml  --record
deployment.apps/nginx created
[root@node-1 k8s.yaml]# kubectl  get  deployments.apps 
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   2/2     2            2           36s
[root@node-1 k8s.yaml]# kubectl  get  rs
NAME               DESIRED   CURRENT   READY   AGE
nginx-86c57db685   2         2         2       41s
[root@node-1 k8s.yaml]# kubectl  get   pod -o  wide 
NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx-86c57db685-grqft   1/1     Running   0          46s   10.244.2.22   node-3   <none>           <none>
nginx-86c57db685-vv7jm   1/1     Running   0          46s   10.244.1.14   node-2   <none>           <none>
############################################################################
[root@node-1 k8s.yaml]# kubectl run  busybox --image=busybox --dry-run=client -o yaml > testHealthz.yaml 
Error: invalid argument "client" for "--dry-run" flag: strconv.ParseBool: parsing "client": invalid syntax
See 'kubectl run --help' for usage.
[root@node-1 k8s.yaml]# kubectl run  busybox --image=busybox --dry-run=1 -o yaml > testHealthz.yaml 
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
[root@node-1 k8s.yaml]# cat   testHealthz.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - image: busybox
    name: busybox
    resources: {}
    args:
    - /bin/sh
    - -c
    - sleep 10; exit 1       # 并添加pod运行指定脚本命令，模拟容器启动10秒后发生故障，退出状态码为1
  dnsPolicy: ClusterFirst
  restartPolicy: OnFailure # 将默认的Always修改为OnFailure
status: {}
[root@node-1 k8s.yaml]# kubectl apply  -f  testHealthz.yaml 
pod/busybox created
[root@node-1 k8s.yaml]# kubectl  get  pod 
NAME                     READY   STATUS              RESTARTS   AGE
busybox                  0/1     ContainerCreating   0          11s
nginx-86c57db685-grqft   1/1     Running             0          31m
nginx-86c57db685-vv7jm   1/1     Running             0          31m
[root@node-1 k8s.yaml]# kubectl  describe  pod  busybox
[root@node-1 k8s.yaml]# kubectl  get  pod  -w 
NAME                     READY   STATUS             RESTARTS   AGE
busybox                  0/1     CrashLoopBackOff   3          3m16s
nginx-86c57db685-grqft   1/1     Running            0          34m
nginx-86c57db685-vv7jm   1/1     Running            0          34m
busybox                  1/1     Running            4          3m49s
busybox                  0/1     Error              4          4m1s
busybox                  0/1     CrashLoopBackOff   4          4m13s
busybox                  1/1     Running            5          5m29s
busybox                  0/1     Error              5          5m41s
busybox                  0/1     CrashLoopBackOff   5          5m54s
[root@node-1 k8s.yaml]# kubectl  delete -f  testHealthz.yaml 
pod "busybox" deleted
[root@node-1 k8s.yaml]# cat   liveness.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness
spec:
  restartPolicy: OnFailure
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 10   # 容器启动 10 秒之后开始检测
      periodSeconds: 5          # 每隔 5 秒再检测一次
[root@node-1 k8s.yaml]# kubectl  apply  -f  liveness.yaml 
[root@node-1 k8s.yaml]# kubectl get pod   -w
NAME                     READY   STATUS    RESTARTS   AGE
liveness                 1/1     Running   0          88s
nginx-86c57db685-grqft   1/1     Running   0          45m
nginx-86c57db685-vv7jm   1/1     Running   0          45m
liveness                 1/1     Running   1          93s
liveness                 1/1     Running   2          3m5s    #restarts 服务不正常重启了几次
[root@node-1 k8s.yaml]# kubectl  describe  pod liveness
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  4m15s                default-scheduler  Successfully assigned default/liveness to node-2
  Normal   Pulling    88s (x3 over 4m13s)  kubelet, node-2    Pulling image "busybox"
  Normal   Pulled     72s (x3 over 3m57s)  kubelet, node-2    Successfully pulled image "busybox"
  Normal   Created    72s (x3 over 3m57s)  kubelet, node-2    Created container liveness
  Normal   Started    71s (x3 over 3m56s)  kubelet, node-2    Started container liveness
  Warning  Unhealthy  29s (x9 over 3m24s)  kubelet, node-2    Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
  Normal   Killing    29s (x3 over 3m14s)  kubelet, node-2    Container liveness failed liveness probe, will be restarted
[root@node-1 k8s.yaml]# cat  readiness.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness
spec:
  restartPolicy: OnFailure
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    readinessProbe:    # 这里将livenessProbe换成readinessProbe即可，其它配置都一样
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 10   # 容器启动 10 秒之后开始检测
      periodSeconds: 5          # 每隔 5 秒再检测一次
[root@node-1 k8s.yaml]# kubectl  apply    -f  readiness.yaml 
[root@node-1 k8s.yaml]# cat  myapp-v1.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mytest
spec:
  replicas: 10     # 这里准备10个数量的pod
  selector:
    matchLabels:
      app: mytest
  template:
    metadata:
      labels:
        app: mytest
    spec:
      containers:
      - name: mytest
        image: busybox
        args:
        - /bin/sh
        - -c
        - sleep 10; touch /tmp/healthy; sleep 30000
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 10
          periodSeconds: 5
[root@node-1 k8s.yaml]# kubectl  apply  -f   myapp-v1.yaml
[root@node-1 k8s.yaml]# cat  myapp-v2.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mytest
spec:
  strategy:
    rollingUpdate:
      maxSurge: 35%   # 滚动更新的副本总数最大值(以10的基数为例)：10 + 10 * 35% = 13.5 --> 14
      maxUnavailable: 35%  # 可用副本数最大值(默认值两个都是25%)： 10 - 10 * 35% = 6.5  --> 7
  replicas: 10
  selector:
    matchLabels:
      app: mytest
  template:
    metadata:
      labels:
        app: mytest
    spec:
      containers:
      - name: mytest
        image: busybox
        args:
        - /bin/sh
        - -c
        - sleep 30000   # 可见这里并没有生成/tmp/healthy这个文件，所以下面的检测必然失败
        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 10
          periodSeconds: 5
[root@node-1 k8s.yaml]# kubectl   apply  -f  myapp-v2.yaml   --record 
deployment.apps/mytest configured
[root@node-1 k8s.yaml]# kubectl   rollout  history  deployment   mytest
deployment.apps/mytest 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl apply --filename=myapp-v2.yaml --record=true
[root@node-1 k8s.yaml]# kubectl  get   deployments.apps   mytest
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
mytest   7/10    7            7           9m13s
[root@node-1 k8s.yaml]# kubectl  describe   deployments.apps mytest 
[root@node-1 k8s.yaml]# kubectl  get   deployments.apps 
mytest  nginx   
[root@node-1 k8s.yaml]# kubectl  get   deployments.apps mytest 
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
mytest   7/10    7            7           11m
[root@node-1 k8s.yaml]# kubectl  edit    deployments.apps mytest 
Edit cancelled, no changes made.
  strategy:
    rollingUpdate:
      maxSurge: 35%
      maxUnavailable: 35%
    type: RollingUpdate
[root@node-1 k8s.yaml]# kubectl describe  deployments.apps mytest  |  grep  Replicas
Replicas:               10 desired | 7 updated | 14 total | 7 available | 7 unavailable
  Available      True    MinimumReplicasAvailable
10+10*35%=13.5=14个
10-10*35%=6.5=7个
[root@node-1 k8s.yaml]# kubectl rollout history deployment mytest    
deployment.apps/mytest 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl apply --filename=myapp-v2.yaml --record=true

[root@node-1 k8s.yaml]# kubectl rollout undo deployment mytest --to-revision=1 
deployment.apps/mytest rolled back
[root@node-1 k8s.yaml]# kubectl  get   pod  -w 
#############################################service############################
[root@node-1 k8s.yaml]# kubectl expose deployment nginx --port=80 --target-port=80 --dry-run=client -o yaml  
Error: invalid argument "client" for "--dry-run" flag: strconv.ParseBool: parsing "client": invalid syntax
See 'kubectl expose --help' for usage.
[root@node-1 k8s.yaml]# kubectl expose deployment nginx --port=80 --target-port=80 --dry-run=1 -o yaml  
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
status:
  loadBalancer: {}
[root@node-1 k8s.yaml]# kubectl expose deployment nginx --port=80 --target-port=80  
Error from server (AlreadyExists): services "nginx" already exists
[root@node-1 k8s.yaml]# kubectl  delete  deployment  nginx  
deployment.apps "nginx" deleted
[root@node-1 k8s.yaml]# kubectl  get   pod 
[root@node-1 k8s.yaml]#  kubectl create deployment nginx --image=nginx  
deployment.apps/nginx created
[root@node-1 k8s.yaml]# kubectl expose deployment nginx --port=80 --target-port=80  
Error from server (AlreadyExists): services "nginx" already exists
[root@node-1 k8s.yaml]# kubectl  get  svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP        2d23h
nginx        NodePort    10.96.95.82   <none>        80:30800/TCP   2d23h
[root@node-1 k8s.yaml]# kubectl  delete  services nginx 
service "nginx" deleted
[root@node-1 k8s.yaml]# kubectl  get  svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   2d23h
[root@node-1 k8s.yaml]# kubectl expose deployment nginx --port=80 --target-port=80  
service/nginx exposed
[root@node-1 k8s.yaml]# kubectl  get  svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   2d23h
nginx        ClusterIP   10.96.13.195   <none>        80/TCP    2s
[root@node-1 k8s.yaml]# kubectl  delete  -f  myapp-v2.yaml 
deployment.apps "mytest" deleted
[root@node-1 k8s.yaml]# kubectl  get  pod 
NAME                     READY   STATUS    RESTARTS   AGE
nginx-86c57db685-8rk9m   1/1     Running   0          4m1s
[root@node-1 k8s.yaml]# kubectl  get  svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   2d23h
nginx        ClusterIP   10.96.13.195   <none>        80/TCP    3m57s
[root@node-1 k8s.yaml]# curl  10.96.13.195 
[root@node-1 k8s.yaml]# kubectl  get   endpoints
NAME         ENDPOINTS         AGE
kubernetes   10.0.1.201:6443   2d23h
nginx        10.244.1.32:80    6m39s
[root@node-1 k8s.yaml]# kubectl  get  pod  -o  wide 
NAME                     READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
nginx-86c57db685-8rk9m   1/1     Running   0          7m30s   10.244.1.32   node-2   <none>           <none>
[root@node-1 k8s.yaml]# kubectl exec -it  nginx-86c57db685-8rk9m  -- bash 
[root@node-1 ~]# kubectl  exec  -it  nginx-86c57db685-pv2j5  -- bash 
root@nginx-86c57db685-8rk9m:/# echo 1111 > /usr/share/nginx/html/index.html
root@nginx-86c57db685-pv2j5:/# echo 3333 > /usr/share/nginx/html/index.html
root@nginx-86c57db685-8rk9m:/# exit
exit
[root@node-1 k8s.yaml]# curl  10.96.13.195 
1111
[root@node-1 k8s.yaml]# kubectl get  svc nginx -o  yaml 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2022-05-23T02:16:22Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "539448"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: 45bd38fc-7b0d-4dd3-9dff-f676c4a9989a
spec:
  clusterIP: 10.96.13.195
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
[root@node-1 k8s.yaml]#  kubectl patch svc nginx -p '{"spec":{"type":"NodePort"}}'
service/nginx patched
[root@node-1 k8s.yaml]# kubectl get  svc nginx -o  yaml 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2022-05-23T02:16:22Z"
  labels:
    app: nginx
  name: nginx
  namespace: default
  resourceVersion: "542199"
  selfLink: /api/v1/namespaces/default/services/nginx
  uid: 45bd38fc-7b0d-4dd3-9dff-f676c4a9989a
spec:
  clusterIP: 10.96.13.195
  externalTrafficPolicy: Cluster
  ports:
  - nodePort: 31675
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: NodePort                #主要改的是这里的类型
status:
  loadBalancer: {}
[root@node-1 k8s.yaml]# kubectl get  svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        3d
nginx        NodePort    10.96.13.195   <none>        80:31675/TCP   19m  #多了端口31675和TYPE类型也改为NodePort
[root@node-1 k8s.yaml]# curl  http://10.0.1.201:31675
1111
[root@node-1 ~]# curl  10.0.1.202:31675 
1111
[root@node-1 ~]# curl  10.0.1.202:31675 
3333
[root@node-1 ~]# curl  10.0.1.203:31675 
1111
[root@node-1 ~]# curl  10.0.1.203:31675 
3333
谷歌浏览器里直接输入http://10.0.1.201:31675/也是可以看到1111
[root@node-1 k8s.yaml]# kubectl  describe  svc  nginx 
Name:                     nginx
Namespace:                default
Labels:                   app=nginx
Annotations:              <none>
Selector:                 app=nginx
Type:                     NodePort
IP:                       10.96.13.195
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  31675/TCP
Endpoints:                10.244.1.32:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
[root@node-1 k8s.yaml]# find  /  -name  kube-proxy
/var/lib/docker/overlay2/bf91d9a3690771ef22bc1e61beac054da51c81e5a69e4cee9b0f3989e06d94b9/diff/usr/local/bin/kube-proxy
/var/lib/docker/overlay2/71a0e967d26c8bdf230265744f9c09cdb61f85eecfdb19e0d37753785a475e5a/diff/var/lib/kube-proxy
/var/lib/docker/overlay2/474ad716b5d827a09d0c49e6a142a2e402c734d7a2fd56595ec4672ce444f0b0/diff/var/lib/kube-proxy
/var/lib/docker/overlay2/474ad716b5d827a09d0c49e6a142a2e402c734d7a2fd56595ec4672ce444f0b0/merged/usr/local/bin/kube-proxy
/var/lib/docker/overlay2/474ad716b5d827a09d0c49e6a142a2e402c734d7a2fd56595ec4672ce444f0b0/merged/var/lib/kube-proxy
/var/lib/kubelet/pods/70778556-8d6e-4e8c-9181-56f0f9ab5441/volumes/kubernetes.io~configmap/kube-proxy
/var/lib/kubelet/pods/70778556-8d6e-4e8c-9181-56f0f9ab5441/containers/kube-proxy
/var/log/pods/kube-system_kube-proxy-dm6ns_70778556-8d6e-4e8c-9181-56f0f9ab5441/kube-proxy
[root@node-1 ~]# cd   /var/lib/kubelet/pods/70778556-8d6e-4e8c-9181-56f0f9ab5441/volumes/kubernetes.io~configmap/kube-proxy 
[root@node-1 kube-proxy]# ls
config.conf  kubeconfig.conf
[root@node-1 kube-proxy]# cat  config.conf 
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  acceptContentTypes: ""
  burst: 0
  contentType: ""
  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
  qps: 0
clusterCIDR: 10.244.0.0/16
configSyncPeriod: 0s
conntrack:
  maxPerCore: null
  min: null
  tcpCloseWaitTimeout: null
  tcpEstablishedTimeout: null
enableProfiling: false
healthzBindAddress: ""
hostnameOverride: ""
iptables:
  masqueradeAll: false
  masqueradeBit: null
  minSyncPeriod: 0s
  syncPeriod: 0s
ipvs:
  excludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: ""
  strictARP: false
  syncPeriod: 0s
kind: KubeProxyConfiguration
metricsBindAddress: ""
mode: ""
nodePortAddresses: null
oomScoreAdj: null
portRange: ""
udpIdleTimeout: 0s
winkernel:
  enableDSR: false
  networkName: ""
  sourceVip: ""
[root@node-1 ~]# kubectl  scale deployment nginx  --replicas=2
deployment.apps/nginx scaled
[root@node-1 ~]# kubectl  get pod  -w 
NAME                     READY   STATUS              RESTARTS   AGE
nginx-86c57db685-8rk9m   1/1     Running             0          38h
nginx-86c57db685-pv2j5   0/1     ContainerCreating   0          8s
nginx-86c57db685-pv2j5   1/1     Running             0          18s
[root@node-1 ~]# kubectl  get pod  -o wide 
NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx-86c57db685-8rk9m   1/1     Running   0          38h   10.244.1.32   node-2   <none>           <none>
nginx-86c57db685-pv2j5   1/1     Running   0          43s   10.244.2.34   node-3   <none>           <none>
[root@node-1 ~]# ipvsadm  -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
[root@node-1 ~]#  kubectl run -it --rm busybox --image=busybox -- sh 
/ # ping nginx.default
